# -*- coding: utf-8 -*-
"""Copy of nnotebook1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ykJJq6oZ_CDcQc9YT0vpfSyeie2MkcGw
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

divyasribhargavi_clean_data_path = kagglehub.dataset_download('divyasribhargavi/clean-data')

print('Data source import complete.')

## This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

#import os
#for dirname, _, filenames in os.walk('/kaggle/input'):
 #   for filename in filenames:
  #      print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd

# Load the dataset
df = pd.read_excel("/content/clean-data.xlsx")

df.head(1)

df.info()

!pip install -q sentence-transformers xgboost

"""## Cleaning Study Title"""

# Fill missing values and preprocess "Study Title"
df['Study Title'] = df['Study Title'].fillna("").str.lower().str.replace(r"[^a-z0-9\s]", "", regex=True)

""" ## Define the function for cleaning interventions"""

# Extract Interventions and classify them
def extract_interventions(intervention_text):
    if pd.isna(intervention_text):
        return {"DRUG": [], "DEVICE": [], "BIOLOGICAL": [], "PROCEDURE": []}

    interventions = {"DRUG": [], "DEVICE": [], "BIOLOGICAL": [], "PROCEDURE": []}
    items = str(intervention_text).split('|')

    for item in items:
        if 'DRUG:' in item:
            interventions["DRUG"].append(item.split('DRUG:')[1].strip().lower())
        elif 'DEVICE:' in item:
            interventions["DEVICE"].append(item.split('DEVICE:')[1].strip().lower())
        elif 'BIOLOGICAL:' in item:
            interventions["BIOLOGICAL"].append(item.split('BIOLOGICAL:')[1].strip().lower())
        elif 'PROCEDURE:' in item:
            interventions["PROCEDURE"].append(item.split('PROCEDURE:')[1].strip().lower())

    return interventions

# Apply classification
df['interventions'] = df['Interventions'].apply(extract_interventions)

df.info()

"""## SBERT for Study Title"""

from sentence_transformers import SentenceTransformer

# Load SBERT model (efficient and lightweight)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for "Study Title"
study_titles = df['Study Title'].tolist()
embeddings = model.encode(study_titles, batch_size=32, show_progress_bar=True)

def get_primary_intervention(interventions):
    for category in ["DRUG", "DEVICE", "BIOLOGICAL", "PROCEDURE"]:
        if interventions[category]:
            return category
    return "OTHER"

df['Primary Intervention'] = df['interventions'].apply(get_primary_intervention)

df.head(1)

df.count()

# List of required columns
required_columns = [
    "nct_id", "Study Title", "Study Results", "Sponsor", "Sex", "Age",
    "Enrollment", "Funder Type", "Time taken for Enrollment",
    "healthy_volunteers", "adult", "child", "older_adult",
    "Primary Intervention"
]

# Create a modified dataset with only the required columns
modified_df = df[required_columns]

modified_df.head()

# Save the modified dataset to Kaggle's working directory for future use
modified_df.to_csv("/kaggle/working/modified_dataset.csv", index=False)

""" ## Study Results"""

# Encode 'Study Results' as 0 (No) and 1 (Yes) using .loc[]
modified_df.loc[:, 'Study Results'] = modified_df['Study Results'].map({'NO': 0, 'YES': 1})

"""## Sponsors"""

# Clean the 'Sponsor' column
def clean_text(text):
    if pd.isna(text):  # Handle missing values
        return ""
    text = text.lower()  # Convert to lowercase
    text = text.strip()  # Remove leading/trailing spaces
    text = text.replace(",", "")  # Remove commas
    text = " ".join(text.split())  # Remove extra spaces
    return text

# Apply cleaning to 'Sponsor' using .loc[]
modified_df.loc[:, 'Sponsor'] = modified_df['Sponsor'].apply(clean_text)

# Display unique cleaned values for verification
print(modified_df['Sponsor'].unique())

from sentence_transformers import SentenceTransformer

# Load SBERT model (efficient and lightweight)
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for 'Sponsor'
sponsor_embeddings = sbert_model.encode(modified_df['Sponsor'].tolist(), batch_size=32, show_progress_bar=True)

# Convert embeddings into a DataFrame for further processing
import numpy as np
sponsor_embeddings_df = pd.DataFrame(sponsor_embeddings, columns=[f'Sponsor_Embedding_{i}' for i in range(sponsor_embeddings.shape[1])])

# Concatenate embeddings back to the modified DataFrame
# modified_df = pd.concat([modified_df.reset_index(drop=True), sponsor_embeddings_df], axis=1)

# Load SBERT model (efficient and lightweight)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for "Study Title"
sponsor_titles = modified_df['Sponsor'].tolist()
sponsor_embeddings = model.encode(study_titles, batch_size=32, show_progress_bar=True)

"""### |-> Creating embeddings for Sponsor and Study Title"""

# Convert embeddings into a DataFrame for further processing
sponsor_embeddings_df = pd.DataFrame(sponsor_embeddings, columns=[f'Sponsor_Embedding_{i}' for i in range(sponsor_embeddings.shape[1])])

# Convert embeddings into a DataFrame for further processing
import numpy as np
study_title_embeddings_df = pd.DataFrame(embeddings, columns=[f'Study_Title_Embedding_{i}' for i in range(embeddings.shape[1])])

"""## Sex"""

# Check unique values in 'Sex'
print("Unique values in 'Sex':", modified_df['Sex'].unique())

# Encode 'Sex' as categorical variables
modified_df.loc[:, 'Sex'] = modified_df['Sex'].map({'ALL': 0, 'MALE': 1, 'FEMALE': 2})

# One-hot encode the "Primary Intervention" column
primary_intervention_encoded = pd.get_dummies(modified_df['Primary Intervention'], prefix='Primary_Intervention')

# Concatenate one-hot encoded columns back to the original DataFrame
modified_df = pd.concat([modified_df, primary_intervention_encoded], axis=1)

# Drop the original "Primary Intervention" column if no longer needed
#df.drop(columns=['Primary Intervention'], inplace=True)

print("One-hot encoding completed successfully!")
print(df.head())

modified_df.head()

"""## Age"""

# Create binary columns for each age group
modified_df['Older_Adult'] = modified_df['Age'].str.contains('OLDER_ADULT', na=False).astype(int)
modified_df['Adult'] = modified_df['Age'].str.contains('ADULT', na=False).astype(int)
modified_df['Child'] = modified_df['Age'].str.contains('CHILD', na=False).astype(int)

# Drop the original 'Age' column if no longer needed
modified_df.drop(columns=['Age'], inplace=True)

"""## Funder_Type"""

# One-hot encode 'Funder Type'
funder_type_encoded = pd.get_dummies(modified_df['Funder Type'], prefix='Funder_Type')

# Concatenate the encoded columns back to the DataFrame
modified_df = pd.concat([modified_df.reset_index(drop=True), funder_type_encoded], axis=1)

# Drop the original column if no longer needed
modified_df.drop(columns=['Funder Type'], inplace=True)

"""## Healthy_Volunteers"""

# Encode 'healthy_volunteers' as 0 (false) and 1 (true)
modified_df.loc[:, 'healthy_volunteers'] = modified_df['healthy_volunteers'].map({'f': 0, 't': 1})

"""## Adult, Older_Adult, Child"""

# Convert t/f to 0/1 for 'adult', 'child', and 'older_adult'
for col in ['adult', 'child', 'older_adult']:
    modified_df.loc[:, col] = modified_df[col].map({'f': 0, 't': 1})

modified_df.head()

# Display all column names in the modified_df DataFrame
print("Column Names in modified_df:")
print(modified_df.columns.tolist())

columns_to_remove = [
    'nct_id', 'Study Results', 'Sponsor','adult', 'child', 'older_adult',
    'Primary Intervention',
    'Enrollment', 'Study Title'
]

final_df = modified_df.drop(columns=columns_to_remove)
print("Columns in final_df after removal:", final_df.columns.tolist())

final_df.head()

final_df.info()

# Concatenate embeddings back to the final DataFrame
final_df_with_embeddings = pd.concat([final_df.reset_index(drop=True), study_title_embeddings_df, sponsor_embeddings_df], axis=1)

from sklearn.model_selection import train_test_split

# Ensure no missing values in target variable ('Time taken for Enrollment')
final_df_with_embeddings = final_df_with_embeddings[final_df_with_embeddings['Time taken for Enrollment'].notnull()]

# Check if 'Primary Intervention' exists and has enough samples for each class; otherwise, skip stratification
if 'Primary Intervention' in final_df_with_embeddings.columns:
    # Get unique values and their counts in 'Primary Intervention'
    value_counts = final_df_with_embeddings['Primary Intervention'].value_counts()

    # Check if all classes have at least 2 samples
    if value_counts.min() >= 2:
        stratify_column = final_df_with_embeddings['Primary Intervention']
    else:
        print("'Primary Intervention' has classes with less than 2 samples. Skipping stratification.")
        stratify_column = None  # Disable stratification
else:
    print("'Primary Intervention' column not found. Skipping stratification.")
    stratify_column = None  # Disable stratification

# Define target variable (Y) and features (X)
X = final_df_with_embeddings.drop(columns=['Time taken for Enrollment'])
y = final_df_with_embeddings['Time taken for Enrollment']

# Perform stratified sampling (or random sampling if stratification is disabled)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=stratify_column, random_state=42
)

print("Train and test sets created successfully!")

# Convert all values in 'Study Title' to strings
#X_train['Study Title'] = X_train['Study Title'].astype(str)
#X_test['Study Title'] = X_test['Study Title'].astype(str) - WE already removed this

from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
#label_encoder = LabelEncoder()

# Fit on combined unique values from both X_train and X_test
#all_study_titles = pd.concat([X_train['Study Title'], X_test['Study Title']]).unique()
#label_encoder.fit(all_study_titles)

# Transform the 'Study Title' column in both datasets
#X_train['Study Title'] = label_encoder.transform(X_train['Study Title'])
#X_test['Study Title'] = label_encoder.transform(X_test['Study Title'])
# Not required

!pip install xgboost

#from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
label_encoder = LabelEncoder()

# 1. Study Title
# X_train['Study Title'] = label_encoder.fit_transform(X_train['Study Title'])

# 2. Sex
X_train['Sex'] = label_encoder.fit_transform(X_train['Sex'])

# 3. healthy_volunteers
X_train['healthy_volunteers'] = X_train['healthy_volunteers'].map({'t': 1, 'f': 0})

# Check data types of X_train
print(X_train.dtypes)

# Identify non-numerical columns
non_numerical_columns = X_train.select_dtypes(include=['object']).columns.tolist()
print("Non-numerical columns:", non_numerical_columns)

#from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
label_encoder = LabelEncoder()

# 1. Study Title
# X_train['Study Title'] = label_encoder.fit_transform(X_train['Study Title'])

# 2. Sex
X_test['Sex'] = label_encoder.fit_transform(X_test['Sex'])

# 3. healthy_volunteers
X_test['healthy_volunteers'] = X_test['healthy_volunteers'].map({'t': 1, 'f': 0})

# Check data types of X_train
print(X_test.dtypes)

# Identify non-numerical columns
non_numerical_columns = X_test.select_dtypes(include=['object']).columns.tolist()
print("Non-numerical columns:", non_numerical_columns)



# Remove 'Study Title' and any other non-numerical columns from X_train and X_test
#columns_to_remove = ['Study Title']  # Add other non-numerical columns if needed
#X_train = X_train.drop(columns=columns_to_remove)
#X_test = X_test.drop(columns=columns_to_remove)

print("Columns in X_train after removal:", X_train.columns.tolist())

# Check data types of X_train
print(X_train.dtypes)

# Identify non-numerical columns
non_numerical_columns = X_train.select_dtypes(include=['object']).columns.tolist()
print("Non-numerical columns:", non_numerical_columns)

# Check data types of X_train
print(X_test.dtypes)

# Identify non-numerical columns
non_numerical_columns = X_test.select_dtypes(include=['object']).columns.tolist()
print("Non-numerical columns:", non_numerical_columns)

# Now create DMatrix objects with enable_categorical=True
dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder

# ... (Your existing code for data preparation and NaN handling) ...

# 1. Create a LabelEncoder object outside the loop:
label_encoder = LabelEncoder()

# Ensure X_test also has the same columns as X_train
for col in X_train.columns:
    if col not in X_test.columns:
        X_test[col] = 0  # or any suitable default value

# Apply label encoding to non-numerical columns in both X_train and X_test
for col in X_train.select_dtypes(include=['object']).columns:
    # Fit on the combined unique values from both X_train and X_test
    all_values = pd.concat([X_train[col], X_test[col]]).unique()
    label_encoder.fit(all_values)  # Fit the encoder on all unique values

    # Transform both X_train and X_test using .loc for assignment
    X_train.loc[:, col] = label_encoder.transform(X_train[col])
    X_test.loc[:, col] = label_encoder.transform(X_test[col])

    # Explicitly convert the encoded column to numeric type in both DataFrames
    X_train.loc[:, col] = pd.to_numeric(X_train[col])
    X_test.loc[:, col] = pd.to_numeric(X_test[col])

# Now create DMatrix objects with enable_categorical=True
dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)

# ... (Rest of your XGBoost training and evaluation code) ...



import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# ... (Your existing code for data preparation and NaN handling) ...

# 1. Create DMatrix objects:
dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)

# 2. Define XGBoost parameters:
params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'tree_method': 'hist',
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'verbosity': 1
}

# 3. Train the model with early stopping:
eval_set = [(dtrain, 'train'), (dtest, 'eval')]
model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    early_stopping_rounds=10,
    evals=eval_set,
    verbose_eval=10
)

# 4. Make predictions:
y_pred = model.predict(dtest)

# 5. Evaluate the model:
#rmse = mean_squared_error(y_test, y_pred, squared=False)
#print(f"RMSE: {rmse}")

# 6. Further analysis (optional):
# - Feature importance:
#   importance = model.get_score(importance_type='gain')
#   print(importance)
# - Visualizations:
#   xgb.plot_importance(model)
#   xgb.plot_tree(model, num_trees=0)

# 4. Make predictions:
y_pred = model.predict(dtest)
print(y_pred)

# 5. Evaluate the model (RMSE):
# rmse = mean_squared_error(y_test, y_pred, squared=False)  # Remove 'squared' argument
rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # Calculate RMSE using NumPy
print(f"RMSE: {rmse}")

# 6. Define a threshold for classification (adapt as needed):
threshold = np.median(y_test)  # Example: Using the median as the threshold

# 7. Convert predictions to classes based on the threshold:
y_pred_classes = np.where(y_pred > threshold, 1, 0)
y_test_classes = np.where(y_test > threshold, 1, 0)

# 8. Calculate accuracy, precision, and F1 score:
from sklearn.metrics import accuracy_score, precision_score, f1_score # Import necessary functions

accuracy = accuracy_score(y_test_classes, y_pred_classes)
precision = precision_score(y_test_classes, y_pred_classes)
f1 = f1_score(y_test_classes, y_pred_classes)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

!pip install matplotlib

import matplotlib.pyplot as plt

# 6. Feature importance:
importance = model.get_score(importance_type='gain')
print("Feature Importance:")
print(importance)

# 7. Feature importance plot:
xgb.plot_importance(model, importance_type='gain')
plt.title('Feature Importance (Gain)')
plt.savefig('feature_imp.png')
plt.show()

# 8. Plot a single tree:
xgb.plot_tree(model, num_trees=0)  # Plot the first tree
fig = plt.gcf()
fig.set_size_inches(150, 100)  # Adjust size as needed
plt.savefig('xgboost_tree.png')  # Save the figure to a file named 'xgboost_tree.png'
plt.show()

import numpy as np
from sklearn.metrics import r2_score

def smape(y_true, y_pred):
    """
    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).

    Args:
        y_true: The actual values.
        y_pred: The predicted values.

    Returns:
        The SMAPE value.
    """
    numerator = np.abs(y_true - y_pred)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    return np.mean(numerator / denominator) * 100

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

# Calculate SMAPE
smape_value = smape(y_test, y_pred)
print(f"SMAPE: {smape_value}")

!pip install shap

import shap
import xgboost as xgb
import matplotlib.pyplot as plt

# Use pred_contribs=True to get SHAP values directly from XGBoost
shap_values = model.predict(dtest, pred_contribs=True)

# The last column contains the bias term, so remove it
shap_values = shap_values[:, :-1]

# Convert to a DataFrame for better visualization
shap_df = pd.DataFrame(shap_values, columns=X_train.columns)

print("SHAP values calculated successfully!")

# One-hot encode categorical features in both X_train and X_test
X_train_encoded = pd.get_dummies(X_train, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, drop_first=True)

# Align columns in X_train_encoded and X_test_encoded
X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1)
X_test_encoded = X_test_encoded.fillna(0)  # Fill missing columns in test set with 0

dtrain_encoded = xgb.DMatrix(X_train_encoded, label=y_train)
dtest_encoded = xgb.DMatrix(X_test_encoded, label=y_test)

params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'tree_method': 'hist',
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'verbosity': 1
}

model = xgb.train(
    params,
    dtrain_encoded,
    num_boost_round=1000,
    early_stopping_rounds=10,
    evals=[(dtrain_encoded, 'train'), (dtest_encoded, 'eval')],
    verbose_eval=10
)

import shap

# Initialize SHAP TreeExplainer
explainer = shap.TreeExplainer(model)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test_encoded)

# Visualize global feature importance
shap.summary_plot(shap_values, X_test_encoded)

import matplotlib.pyplot as plt
import shap

# Visualize global feature importance and save the plot
shap.summary_plot(shap_values, X_test_encoded, show=False)
plt.savefig('/content/shap_summary_plot.png')  # Save as PNG
plt.savefig('/content/shap_summary_plot.pdf')  # Save as PDF (optional)

print("SHAP summary plot saved successfully!")

plt.figure(figsize=(8, 6))  # Adjust figure size if needed
plt.scatter(y_test, y_pred, alpha=0.5)  # Create scatter plot
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values")

   # Add a diagonal line for reference (perfect predictions)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.savefig('actual_vs_predicted.png')  # Save the plot
plt.show()